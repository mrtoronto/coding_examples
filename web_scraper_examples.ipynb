{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Packages</a></span></li><li><span><a href=\"#Article-Abstract-Data\" data-toc-modified-id=\"Article-Abstract-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Article Abstract Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-xml-package\" data-toc-modified-id=\"Using-xml-package-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Using <code>xml</code> package</a></span></li><li><span><a href=\"#Using-lxml-package\" data-toc-modified-id=\"Using-lxml-package-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Using <code>lxml</code> package</a></span></li><li><span><a href=\"#Extracting-PMC-Article-Link-from-Article-Abstract-Page\" data-toc-modified-id=\"Extracting-PMC-Article-Link-from-Article-Abstract-Page-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Extracting PMC Article Link from Article Abstract Page</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Guide\n",
    "\n",
    "## Import Packages\n",
    "\n",
    "We'll use `requests` to pull the data and `lxml` and/or `xml` to parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:32:35.746786Z",
     "start_time": "2019-12-01T15:32:35.358141Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This article](https://docs.python-guide.org/scenarios/scrape/) is a concise summary of some helpful ideas. Below I'll do it to a pubmed article and I can add other examples as time goes on. \n",
    "\n",
    "## Article Abstract Data\n",
    "\n",
    "### Using `xml` package\n",
    "\n",
    "Below I'll pull the abstract from a PubMed [article](https://www.ncbi.nlm.nih.gov/pubmed/12734240) using the `xml` package. There's always lots of ways to do stuff in python so I'm going to show you two different scraping packages so you can compare and choose the one the makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:37:16.614806Z",
     "start_time": "2019-12-01T15:37:16.043210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\">\\n    <head xmlns:xi=\"http://www.w3.org/2001/XInclude\"><meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\\n    <!-- meta -->\\n    <meta name=\"author\" content=\"pubmeddev\" /><meta name=\"keywords\" content=\"PubMed, National Center for Biotechnology Information, NCBI, United States National Library of Medicine, NLM, MEDLINE, Medical Journals, pub med, Entrez, Journal Articles, Citation search\" /><meta name=\"description\" content=\"PubMed comprises more than 30 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full-text content from PubMed Central and publisher web sites.\" /><meta name=\"robots\" content=\"index,nofollow,noarchive\" /><meta property=\"og:image\" content=\"http://www.ncbi.nlm.nih.gov/coreutils/img/pubmed256blue.png\" /><meta property=\"og:image:secure_url\" content=\"https://www.ncbi.nlm.nih.gov/coreutils/img/pubmed256blue.png\" />\\n<meta name=\"ncbi_app\" content=\"entrez\" /><meta name=\"ncbi_db\" content=\"pubmed\" /><meta name=\"ncbi_report\" content=\"abstract\" /><meta name=\"ncbi_format\" content=\"html\" /><meta name=\"ncbi_pagesize\" content=\"20\" /><meta name=\"ncbi_sortorder\" content=\"default\" /><meta name=\"ncbi_pageno\" content=\"1\" /><meta name=\"ncbi_resultcount\" content=\"1\" /><meta name=\"ncbi_op\" content=\"retrieve\" /><meta name=\"ncbi_pdid\" content=\"abstract\" /><meta name=\"ncbi_sessionid\" content=\"CE8C8D2ADE3DE2C1_0680SID\" /><meta name=\"ncbi_uidlist\" content=\"12734240\" /><meta name=\"ncbi_filter\" content=\"all\" /><meta name=\"ncbi_stat\" content=\"false\" /><meta name=\"ncbi_hitstat\" content=\"false\" />\\n\\n    \\n    <!-- title -->\\n    <title>Usefulness of the MicroSeq 500 16S ribosomal DNA-based bacterial identification system for identification of clinically significant b'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save the URL as a variable\n",
    "url='https://www.ncbi.nlm.nih.gov/pubmed/12734240'\n",
    "\n",
    "### Using the package `requests` ping the page and save the response as an object.\n",
    "### requests docs : https://requests.readthedocs.io/en/master/user/quickstart/#response-content\n",
    "requests_object = requests.get(url)\n",
    "\n",
    "### Take the page's text from our requests object\n",
    "requests_object_content = requests_object.text\n",
    "\n",
    "requests_object_content[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:37:17.305723Z",
     "start_time": "2019-12-01T15:37:17.298185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element '{http://www.w3.org/1999/xhtml}head' at 0x111cd27c8>,\n",
       " <Element '{http://www.w3.org/1999/xhtml}body' at 0x111d6f408>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Using the `xml` package, we can convert the gross string above into a python data structure that \n",
    "### we can easily flip through.\n",
    "### xml docs : https://docs.python.org/3.3/library/xml.etree.elementtree.html\n",
    "root = ET.fromstring(requests_object_content)\n",
    "### The `root` is the root of the xml file and then we can find elements beneath that l\n",
    "root.findall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:37:18.366188Z",
     "start_time": "2019-12-01T15:37:18.361255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element '{http://www.w3.org/1999/xhtml}title' at 0x111d66c78>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.findall('./{http://www.w3.org/1999/xhtml}head/{http://www.w3.org/1999/xhtml}title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:37:20.264906Z",
     "start_time": "2019-12-01T15:37:20.257394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element 'head' at 0x111cd27c8>, <Element 'body' at 0x111d6f408>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This isn't a common thing but this `{http://www.w3.org/1999/xhtml}` prefix thing on each element is super\n",
    "### annoying so I found some code that'll just remove that from all of the element tree.\n",
    "for elem in root.getiterator():\n",
    "    if not hasattr(elem.tag, 'find'): continue\n",
    "    i = elem.tag.find('}')\n",
    "    if i >= 0:\n",
    "        elem.tag = elem.tag[i+1:]\n",
    "root.findall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T15:37:22.413042Z",
     "start_time": "2019-12-01T15:37:22.408419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Due to the inadequate automation in the amplification and sequencing procedures, the use of 16S rRNA gene sequence-based methods in clinical microbiology laboratories is largely limited to identification of strains that are difficult to identify by phenotypic methods. In this study, using conventional full-sequence 16S rRNA gene sequencing as the \"gold standard,\" we evaluated the usefulness of the MicroSeq 500 16S ribosomal DNA (rDNA)-based bacterial identification system, which involves amplification and sequencing of the first 527-bp fragment of the 16S rRNA genes of bacterial strains and analysis of the sequences using the database of the system, for identification of clinically significant bacterial isolates with ambiguous biochemical profiles. Among 37 clinically significant bacterial strains that showed ambiguous biochemical profiles, representing 37 nonduplicating aerobic gram-positive and gram-negative, anaerobic, and Mycobacterium species, the MicroSeq 500 16S rDNA-based bacterial identification system was successful in identifying 30 (81.1%) of them. Five (13.5%) isolates were misidentified at the genus level (Granulicatella adiacens was misidentified as Abiotrophia defectiva, Helcococcus kunzii was misidentified as Clostridium hastiforme, Olsenella uli was misidentified as Atopobium rimae, Leptotrichia buccalis was misidentified as Fusobacterium mortiferum, and Bergeyella zoohelcum was misidentified as Rimerella anatipestifer), and two (5.4%) were misidentified at the species level (Actinomyces odontolyticus was misidentified as Actinomyces meyeri and Arcobacter cryaerophilus was misidentified as Arcobacter butzleri). When the same 527-bp DNA sequences of these seven isolates were compared to the known 16S rRNA gene sequences in the GenBank, five yielded the correct identity, with good discrimination between the best and second best match sequences, meaning that the reason for misidentification in these five isolates was due to a lack of the 16S rRNA gene sequences of these bacteria in the database of the MicroSeq 500 16S rDNA-based bacterial identification system. In conclusion, the MicroSeq 500 16S rDNA-based bacterial identification system is useful for identification of most clinically important bacterial strains with ambiguous biochemical profiles, but the database of the MicroSeq 500 16S rDNA-based bacterial identification system has to be expanded in order to encompass the rarely encountered bacterial species and achieve better accuracy in bacterial identification.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### If you check out the articles page, right click and inspect element you can see how its all set up. \n",
    "### I like to use XPATH to select stuff while web scraping. You can use less specific stuff but it tends to \n",
    "### difficult to use in real situations.\n",
    "### XPATH docs : \n",
    "\n",
    "list(root.findall('.//div[@class=\"rprt_all\"]//div[@class=\"abstr\"]/')[1].itertext())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
